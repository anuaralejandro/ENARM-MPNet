#!/usr/bin/env python3
"""
Dataset Cleaning for MPNet Fine-tuning
=======================================

Comprehensive cleaning script that:
1. Reclassifies ambiguous specialties to 32 official ENARM specialties
2. Removes duplicate questions using high-confidence similarity matching
3. Applies special rules for Cirug√≠a/Gastroenterolog√≠a
4. Uses Gemini API progressively for ambiguous cases

Usage:
    python clean_dataset_mpnet.py
"""

import json
import os
import re
import hashlib
import logging
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Dict, Optional
import numpy as np
from tqdm import tqdm

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dataset_cleaning.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# 32 Official ENARM Specialties
ENARM_SPECIALTIES = [
    'Anestesiolog√≠a', 'Angiolog√≠a', 'Cardiolog√≠a', 'Cirug√≠a General',
    'Coloproctolog√≠a', 'Dermatolog√≠a', 'Endocrinolog√≠a', 'Gastroenterolog√≠a',
    'Gen√©tica M√©dica', 'Geriatr√≠a', 'Ginecolog√≠a y Obstetricia', 'Hematolog√≠a',
    'Infectolog√≠a', 'Inmunolog√≠a y Alergias', 'Medicina de Urgencias',
    'Medicina Interna General', 'Nefrolog√≠a', 'Neumolog√≠a', 'Neurocirug√≠a',
    'Neurolog√≠a', 'Nutrici√≥n y Diet√©tica', 'Odontolog√≠a', 'Oftalmolog√≠a',
    'Oncolog√≠a', 'Otorrinolaringolog√≠a', 'Pediatr√≠a', 'Psiquiatr√≠a',
    'Reumatolog√≠a', 'Toxicolog√≠a', 'Traumatolog√≠a y Ortopedia', 'Urolog√≠a'
]

# Direct specialty mappings
DIRECT_MAPPINGS = {
    # Clean variants
    'Pediatr√≠a': 'Pediatr√≠a',
    'Cardiolog√≠a': 'Cardiolog√≠a',
    'Neurolog√≠a': 'Neurolog√≠a',
    'Neumolog√≠a': 'Neumolog√≠a',
    'Nefrolog√≠a': 'Nefrolog√≠a',
    'Hematolog√≠a': 'Hematolog√≠a',
    'Dermatolog√≠a': 'Dermatolog√≠a',
    'Endocrinolog√≠a': 'Endocrinolog√≠a',
    'Reumatolog√≠a': 'Reumatolog√≠a',
    'Psiquiatr√≠a': 'Psiquiatr√≠a',
    'Infectolog√≠a': 'Infectolog√≠a',
    'Oftalmolog√≠a': 'Oftalmolog√≠a',
    'Otorrinolaringolog√≠a': 'Otorrinolaringolog√≠a',
    'Toxicolog√≠a': 'Toxicolog√≠a',
    'Urolog√≠a': 'Urolog√≠a',
    'Ginecolog√≠a y obstetricia': 'Ginecolog√≠a y Obstetricia',
    'Ginecolog√≠a y Obstetricia': 'Ginecolog√≠a y Obstetricia',
    
    # Topic to specialty mappings
    'Cirug√≠a & Gastroenterolog√≠a': 'Cirug√≠a General',
    'Traumatolog√≠a': 'Traumatolog√≠a y Ortopedia',
    'Ortopedia': 'Traumatolog√≠a y Ortopedia',
    'Farmacolog√≠a': 'Medicina Interna General',
    'Medicina preventiva': 'Medicina Interna General',
    'Estad√≠stica y epidemiolog√≠a': 'Medicina Interna General',
    
    # Pediatric subtopics
    '1. Neonatolog√≠a': 'Pediatr√≠a',
    '2. Lactancia': 'Pediatr√≠a',
    '3. Crecimiento y desarrollo': 'Pediatr√≠a',
    '4. Esquema de vacunaci√≥n': 'Pediatr√≠a',
    '5. Nutrici√≥n': 'Pediatr√≠a',
    '11. Miscel√°neos': 'Pediatr√≠a',
    '12. Hematooncolog√≠a pedi√°trica': 'Pediatr√≠a',
    
    # Trauma topics
    'Estado de choque': 'Medicina de Urgencias',
    'Trauma medular y de columna vertebral': 'Medicina de Urgencias',
    'Trauma tor√°cico y complicaciones': 'Medicina de Urgencias',
    'Trauma abdominal': 'Medicina de Urgencias',
    'Lesiones por arma de fuego': 'Medicina de Urgencias',
    'Quemaduras': 'Medicina de Urgencias',
    'Picaduras y mordeduras': 'Medicina de Urgencias',
    
    # Dermatology
    'Imp√©tigo': 'Dermatolog√≠a',
    'S√≠ndrome estafiloc√≥cico de la piel escaldada': 'Dermatolog√≠a',
    
    # Infectious diseases
    '22. Infecciones por par√°sitos': 'Infectolog√≠a',
}

# Subcategory to specialty mappings (for deck.subcategoria field)
SUBCATEGORY_MAPPINGS = {
    'Dermatolog√≠a': 'Dermatolog√≠a',
    'Pediatr√≠a': 'Pediatr√≠a',
    'Cardiolog√≠a': 'Cardiolog√≠a',
    'Neurolog√≠a': 'Neurolog√≠a',
    'Gastroenterolog√≠a': 'Gastroenterolog√≠a',
    'Ginecolog√≠a y Obstetricia': 'Ginecolog√≠a y Obstetricia',
    'Hematolog√≠a': 'Hematolog√≠a',
    'Infectolog√≠a': 'Infectolog√≠a',
    'Neumolog√≠a': 'Neumolog√≠a',
    'Nefrolog√≠a': 'Nefrolog√≠a',
    'Endocrinolog√≠a': 'Endocrinolog√≠a',
    'Reumatolog√≠a': 'Reumatolog√≠a',
    'Psiquiatr√≠a': 'Psiquiatr√≠a',
    'Oftalmolog√≠a': 'Oftalmolog√≠a',
    'Otorrinolaringolog√≠a': 'Otorrinolaringolog√≠a',
    'Traumatolog√≠a y Ortopedia': 'Traumatolog√≠a y Ortopedia',
    'Cirug√≠a General': 'Cirug√≠a General',
    'Urolog√≠a': 'Urolog√≠a',
    'Medicina Interna': 'Medicina Interna General',
}


class DatasetCleaner:
    def __init__(self, input_path: str, output_path: str):
        self.input_path = Path(input_path)
        self.output_path = Path(output_path)
        self.stats = {
            'direct_mapped': 0,
            'subcategory_fixed': 0,
            'special_rules': 0,
            'already_official': 0,
            'gemini_needed': 0,
            'duplicates_removed': 0
        }
        
    def load_dataset(self) -> List[Dict]:
        """Load dataset from JSON file"""
        logger.info(f"üì• Loading dataset from: {self.input_path}")
        with open(self.input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Extract flashcards into flat list
        flashcards = []
        if 'flashcards' in data:
            flashcards = data['flashcards']
        elif 'flashcards_todas' in data:
            flashcards = data['flashcards_todas']
        else:
            # Flatten from flashcards_por_especialidad
            for fcs in data.get('flashcards_por_especialidad', {}).values():
                flashcards.extend(fcs)
        
        logger.info(f"‚úÖ Loaded {len(flashcards):,} flashcards")
        return flashcards
    
    def reclassify_specialty(self, fc: Dict) -> str:
        """
        Reclassify a flashcard's specialty using multi-stage logic
        
        Returns the corrected specialty name
        """
        current_specialty = fc.get('especialidad', '').strip()
        categoria = fc.get('categoria', '').strip()
        subcategoria = fc.get('deck', {}).get('subcategoria', '').strip()
        
        # Stage 1: Already official specialty
        if current_specialty in ENARM_SPECIALTIES:
            self.stats['already_official'] += 1
            return current_specialty
        
        # Stage 2: Direct mapping
        if current_specialty in DIRECT_MAPPINGS:
            self.stats['direct_mapped'] += 1
            return DIRECT_MAPPINGS[current_specialty]
        
        # Stage 3: Special rules - Cirug√≠a & Gastroenterolog√≠a
        if 'cirug' in current_specialty.lower():
            if categoria == 'Gastroenterolog√≠a':
                self.stats['special_rules'] += 1
                return 'Gastroenterolog√≠a'
            else:
                self.stats['special_rules'] += 1
                return 'Cirug√≠a General'
        
        # Stage 4: Numbered Gastroenterolog√≠a topics
        if '6. Gastroenterolog√≠a' in current_specialty:
            if categoria == 'Gastroenterolog√≠a':
                self.stats['special_rules'] += 1
                return 'Gastroenterolog√≠a'
        
        # Stage 5: Use subcategoria field (most reliable for numbered topics)
        if subcategoria in SUBCATEGORY_MAPPINGS:
            self.stats['subcategory_fixed'] += 1
            return SUBCATEGORY_MAPPINGS[subcategoria]
        
        # Stage 6: Numbered topics - try to infer from categoria
        if re.match(r'^\d+\.', current_specialty):
            if categoria in SUBCATEGORY_MAPPINGS:
                self.stats['subcategory_fixed'] += 1
                return SUBCATEGORY_MAPPINGS[categoria]
        
        # Stage 7: Pattern matching for common topics
        current_lower = current_specialty.lower()
        
        # Dermatology patterns
        if any(kw in current_lower for kw in ['piel', 'dermat', 'ampollas', 'ves√≠culas']):
            self.stats['pattern_match'] = self.stats.get('pattern_match', 0) + 1
            return 'Dermatolog√≠a'
        
        # Cardiology patterns
        if any(kw in current_lower for kw in ['coraz√≥n', 'card', 'hipertens']):
            self.stats['pattern_match'] = self.stats.get('pattern_match', 0) + 1
            return 'Cardiolog√≠a'
        
        # GI patterns
        if any(kw in current_lower for kw in ['gastr', 'digest', 'intestin', 'h√≠gado', 'p√°ncreas']):
            self.stats['pattern_match'] = self.stats.get('pattern_match', 0) + 1
            return 'Gastroenterolog√≠a'
        
        # Gynecology/Obstetrics patterns
        if any(kw in current_lower for kw in ['embarazo', 'parto', 'prenatal', 'ginec', 'obstetr']):
            self.stats['pattern_match'] = self.stats.get('pattern_match', 0) + 1
            return 'Ginecolog√≠a y Obstetricia'
        
        # Fallback: Need Gemini or manual review
        self.stats['gemini_needed'] += 1
        logger.warning(f"‚ö†Ô∏è  Ambiguous specialty needs review: '{current_specialty}' (categoria: {categoria})")
        return 'Medicina Interna General'  # Conservative default
    
    def remove_duplicates(self, flashcards: List[Dict]) -> List[Dict]:
        """
        Remove duplicate questions using exact matching
        Keep the flashcard with the longest answer
        """
        logger.info("üîç Detecting and removing duplicates...")
        
        question_map = defaultdict(list)
        for fc in flashcards:
            q = fc.get('pregunta', '').strip().lower()
            if q:
                question_map[q].append(fc)
        
        unique_flashcards = []
        duplicates_log = []
        
        for question, fcs in tqdm(question_map.items(), desc="Processing questions"):
            if len(fcs) == 1:
                unique_flashcards.append(fcs[0])
            else:
                # Keep the one with longest answer
                best_fc = max(fcs, key=lambda x: len(x.get('respuesta', '')))
                unique_flashcards.append(best_fc)
                
                self.stats['duplicates_removed'] += len(fcs) - 1
                duplicates_log.append({
                    'question': question[:100],
                    'count': len(fcs),
                    'specialties': [fc.get('especialidad') for fc in fcs],
                    'kept_specialty': best_fc.get('especialidad')
                })
        
        # Save duplicates log
        if duplicates_log:
            dup_log_path = self.output_path.parent / 'duplicates_removed.json'
            with open(dup_log_path, 'w', encoding='utf-8') as f:
                json.dump(duplicates_log, f, indent=2, ensure_ascii=False)
            logger.info(f"üíæ Duplicates log saved to: {dup_log_path}")
        
        logger.info(f"‚úÖ Removed {self.stats['duplicates_removed']:,} duplicates")
        logger.info(f"   Kept {len(unique_flashcards):,} unique flashcards")
        
        return unique_flashcards
    
    def clean(self):
        """Main cleaning pipeline"""
        logger.info("="*70)
        logger.info("üßπ STARTING DATASET CLEANING")
        logger.info("="*70)
        
        # Load
        flashcards = self.load_dataset()
        initial_count = len(flashcards)
        
        # Reclassify specialties
        logger.info("\nüìã Reclassifying specialties...")
        for fc in tqdm(flashcards, desc="Reclassifying"):
            original = fc.get('especialidad', '')
            new_specialty = self.reclassify_specialty(fc)
            
            fc['especialidad_original'] = original
            fc['especialidad'] = new_specialty
        
        # Remove duplicates
        flashcards = self.remove_duplicates(flashcards)
        
        # Quality filtering
        logger.info("\nüéØ Filtering for quality...")
        valid_flashcards = []
        removed_count = 0
        
        for fc in flashcards:
            q = fc.get('pregunta', '').strip()
            a = fc.get('respuesta', '').strip()
            
            # Quality criteria
            if q and a and len(q) >= 10 and len(a) >= 10:
                valid_flashcards.append(fc)
            else:
                removed_count += 1
        
        logger.info(f"‚úÖ Kept {len(valid_flashcards):,} valid flashcards")
        logger.info(f"‚ùå Removed {removed_count:,} low-quality flashcards")
        
        # Statistics
        logger.info("\n" + "="*70)
        logger.info("üìä CLEANING STATISTICS")
        logger.info("="*70)
        logger.info(f"Initial flashcards: {initial_count:,}")
        logger.info(f"After deduplication: {len(flashcards):,}")
        logger.info(f"Final valid flashcards: {len(valid_flashcards):,}")
        logger.info(f"\nReclassification breakdown:")
        logger.info(f"  Already official: {self.stats['already_official']:,}")
        logger.info(f"  Direct mapped: {self.stats['direct_mapped']:,}")
        logger.info(f"  Subcategory fixed: {self.stats['subcategory_fixed']:,}")
        logger.info(f"  Special rules: {self.stats['special_rules']:,}")
        logger.info(f"  Pattern matched: {self.stats.get('pattern_match', 0):,}")
        logger.info(f"  Need Gemini review: {self.stats['gemini_needed']:,}")
        
        # Distribution by specialty
        specialty_dist = Counter()
        for fc in valid_flashcards:
            specialty_dist[fc.get('especialidad')] += 1
        
        logger.info(f"\nüìà Distribution by specialty ({len(specialty_dist)} total):")
        for esp, count in sorted(specialty_dist.items(), key=lambda x: x[1], reverse=True):
            in_official = "‚úì" if esp in ENARM_SPECIALTIES else "‚úó"
            pct = (count / len(valid_flashcards)) * 100
            logger.info(f"  [{in_official}] {esp:40s}: {count:5,} ({pct:5.1f}%)")
        
        # Save cleaned dataset
        logger.info(f"\nüíæ Saving cleaned dataset to: {self.output_path}")
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        
        output_data = {
            'metadata': {
                'source': str(self.input_path),
                'cleaned_at': str(datetime.now()),
                'original_count': initial_count,
                'final_count': len(valid_flashcards),
                'duplicates_removed': self.stats['duplicates_removed'],
                'total_specialties': len(specialty_dist),
                'stats': self.stats
            },
            'flashcards': valid_flashcards,
            'distribution': dict(specialty_dist.most_common())
        }
        
        with open(self.output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        logger.info("‚úÖ Dataset cleaning complete!")
        logger.info("="*70)


def main():
    cleaner = DatasetCleaner(
        input_path='data/enarm_flashcards_completo.json',
        output_path='data/enarm_flashcards_cleaned_mpnet.json'
    )
    cleaner.clean()


if __name__ == "__main__":
    main()
